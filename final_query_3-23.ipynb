{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cb072f",
   "metadata": {
    "id": "89172c27"
   },
   "source": [
    "#  Query Gaia for WDS entries - parallelized for multiprocessing across multiple cores\n",
    "#### Summer 2022 -> revised in Spring 2023\n",
    "#### This code should just be run in order and it will do its thing\n",
    "#### Daphne Zakarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694ee351",
   "metadata": {
    "id": "1887a165"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: astropy.extern.six will be removed in 4.0, use the six module directly if it is still needed [astropy.extern.six]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TAP+ (v1.0.1) - Connection:\n",
      "\tHost: gea.esac.esa.int\n",
      "\tUse HTTPS: False\n",
      "\tPort: 80\n",
      "\tSSL Port: 443\n"
     ]
    }
   ],
   "source": [
    "from astropy.io import ascii\n",
    "from astropy.table import vstack, Table, unique\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy import table, log\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord, Distance, Angle\n",
    "from astropy.time import Time\n",
    "from astroquery.gaia import Gaia\n",
    "from astroquery.utils.tap.model import job\n",
    "from itertools import combinations\n",
    "import multiprocessing\n",
    "from multiprocessing import Queue, Pool, freeze_support, Process\n",
    "import os\n",
    "from IPython.display import display\n",
    "from multiprocessing import set_start_method\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4493195",
   "metadata": {
    "id": "b155dd96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## query_gaia(coordinate, radius)\n",
    " pretty self-explanatory... this makes the gaia query for each coordinate in the WDS, and searches for all objects (that fit the parallax and parallax error criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c0cfae",
   "metadata": {
    "id": "5d1b1392",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_gaia(coordinate, radius):\n",
    "    \n",
    "    # This is an ADQL query to the ESA Gaia Archive of Gaia DR3\n",
    "\n",
    "    # these column names list the info to pull from Gaia\n",
    "    # if you change this, make sure to change the wds_to_gaia_query() function\n",
    "    # to update that info in the tables themselves!!\n",
    "    column_names = ['source_id', 'ref_epoch', 'ra', 'ra_error', 'dec',\n",
    "        'dec_error', 'parallax', 'parallax_error', 'parallax_over_error','pmra',\n",
    "        'pmra_error', 'pmdec', 'pmdec_error',\n",
    "        'radial_velocity', 'radial_velocity_error',\n",
    "        'astrometric_params_solved', 'visibility_periods_used',\n",
    "        'astrometric_sigma5d_max','ruwe',\n",
    "        'phot_g_mean_mag', 'phot_g_mean_flux_over_error',\n",
    "        'phot_bp_mean_mag', 'phot_bp_mean_flux_over_error',\n",
    "        'phot_rp_mean_mag', 'phot_rp_mean_flux_over_error',\n",
    "        'bp_rp','phot_bp_rp_excess_factor']\n",
    "\n",
    "    # the columns have to be a string, not a list\n",
    "    # this turns the column list into a string for the query\n",
    "    columns = ''\n",
    "    for column in column_names:\n",
    "        columns += column + ', '\n",
    "    columns =  columns.rstrip(columns[-4])\n",
    "    columns = columns[:len(columns)-2]\n",
    "    columns\n",
    "\n",
    "    # get the degree value for coordinate and radius\n",
    "    ra = coordinate.ra.deg\n",
    "    dec = coordinate.dec.deg\n",
    "    radius = float(radius.to_value(u.deg))\n",
    "\n",
    "    # ADQL query base:\n",
    "    query_base = \"\"\"\n",
    "    SELECT {columns}\n",
    "    FROM gaiadr3.gaia_source\n",
    "    WHERE parallax > 1\n",
    "    AND parallax_over_error > 5\n",
    "    AND parallax_error < 2\n",
    "    AND 1 = CONTAINS(\n",
    "    POINT({ra}, {dec}),\n",
    "    CIRCLE(ra, dec, {rad}))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # format the query with our specific info\n",
    "    query = query_base.format(columns=columns, ra=ra, dec=dec, rad=radius)\n",
    "\n",
    "    # make the query to gaia and save the results into astropy table\n",
    "    job = Gaia.launch_job_async(query)\n",
    "    job\n",
    "    results = job.get_results()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a027c",
   "metadata": {
    "id": "71fc4bed"
   },
   "source": [
    "\n",
    "## test queries for individual rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45122bdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1681972114214,
     "user": {
      "displayName": "Daphne Zakarian",
      "userId": "08361727160766317010"
     },
     "user_tz": 300
    },
    "id": "8dcae88c-204d-4d3e-a981-7726a48673ae",
    "outputId": "c63f60bd-98da-4b2f-e36e-9c93ab1e362d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This fails as the 'query_results_table' is not defined yet. It shows up in \n",
    "# wds_in_gaia_query\n",
    "# print('The query')\n",
    "# query_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ecabb",
   "metadata": {
    "id": "46966bbf",
    "outputId": "cc56d321-9986-4c68-95a6-079e245ac1f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Read in the WDS table\n",
    "# # vayu's lab comp\n",
    "# # path = 'C:/Users/sc36/Documents/DaphneUSNO/NOFS copy-20230218T215456Z-001/NOFS copy/wdstab6-27.ecsv'\n",
    "\n",
    "# # wiser's lab comp\n",
    "# path = '/home/student/djz7128/djz_NOFS/wdstab6-27.ecsv'\n",
    "\n",
    "\n",
    "# wdstab = Table.read(path, header_start=0, data_start=1)\n",
    "\n",
    "\n",
    "# rownum = 100\n",
    "\n",
    "# #read in the coordinates of the primary and secondary in WDS for the designated row number\n",
    "# ra1, dec1 = wdstab['RApri-prepped'][rownum], wdstab['DECpri-prepped'][rownum]\n",
    "# ra2, dec2 = wdstab['RAsec-prepped'][rownum], wdstab['DECsec-prepped'][rownum]\n",
    "# # radius is in degrees\n",
    "# radius = 5*u.arcsec\n",
    "# coord1 = SkyCoord(ra=ra1 , dec = dec1, unit='deg')\n",
    "# myquery1 = query_gaia(coordinate=coord1, radius=radius)\n",
    "\n",
    "# radius = 5*u.arcsec\n",
    "# coord2 = SkyCoord(ra=ra2 , dec = dec2, unit='deg')\n",
    "# myquery2 = query_gaia(coordinate=coord2, radius=radius)\n",
    "\n",
    "# # view the query results stacked together (there may be repeated objects if they are found by both queries)\n",
    "# vstack([myquery1, myquery2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686b804",
   "metadata": {
    "id": "2984fc72"
   },
   "source": [
    "## wds_in_gaia_query(core_num, total_cores) --- query WDS entries in Gaia and save results in a table\n",
    "\n",
    "\n",
    "- The WDS is split up between the number of cores available, and we call this function for each separate instance in the multiprocessing.\n",
    "\n",
    "- It goes through a portion of the WDS, makes a query around the WDS targets, and saves the results in an output table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e469b2ac",
   "metadata": {
    "id": "ed417caf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wds_in_gaia_query(core_num, total_cores): # core num starts at 0\n",
    "\n",
    "    # this function queries gaia for the objects within a 5 arcsec radius of the component coordinates in the WDS\n",
    "\n",
    "    #notes: there are commented out checkpoints used to troubleshoot this function\n",
    "\n",
    "    ## vayu's lab comp\n",
    "    # path = 'C:/Users/sc36/Documents/DaphneUSNO/NOFS copy-20230218T215456Z-001/NOFS copy/wdstab6-27.ecsv'\n",
    "\n",
    "    # save_path = '/home/student/djz7128/djz_NOFS/QueryResults'\n",
    "    # As in other notebooks for WDS_Gaia change the hard-wired paths and use\n",
    "    # os.getcwd()\n",
    "    save_path = os.getcwd()\n",
    "    # the wds table file\n",
    "    path = save_path+'/wdstab6-27.ecsv'\n",
    "\n",
    "\n",
    "\n",
    "    wdstab = Table.read(path, header_start=0, data_start=1)\n",
    "\n",
    "    # total number of queries will be the number of wds entries that we look at\n",
    "    total_num_queries = len(wdstab)\n",
    "\n",
    "    print('Variable total_num_queries is: ',total_num_queries)\n",
    "    \n",
    "    # find approx # of queries per core... ignoring the fraction\n",
    "    queries_per_core = total_num_queries // total_cores\n",
    "    leftover_rows = total_num_queries % total_cores\n",
    "\n",
    "\n",
    "    # make a list of the start and end row variables\n",
    "    start_row_list = []\n",
    "    end_row_list = []\n",
    "\n",
    "    # make a list to get the start and end row for each process (core)\n",
    "    rownum_counter = 0\n",
    "    for core in range(total_cores):\n",
    "        start_row_list.append(rownum_counter)\n",
    "        rownum_counter += queries_per_core\n",
    "        if core == total_cores - 1:\n",
    "            end_row_list.append(total_num_queries)\n",
    "        else:\n",
    "            end_row_list.append(rownum_counter)\n",
    "    end_row_list[-1] = end_row_list[-1] + leftover_rows\n",
    "\n",
    "\n",
    "    # define start and end row of wds for this specific process\n",
    "    # start row is included in query, but end row is not included in the range\n",
    "    wds_start_row =  start_row_list[core_num]\n",
    "    wds_end_row = end_row_list[core_num]\n",
    "\n",
    "    # All of the processes run simultaneously\n",
    "    # This print statement updates the progress for the process\n",
    "    print('core: ', core_num, 'end row = ', wds_end_row)\n",
    "\n",
    "    # these are the column names that have a number data type...\n",
    "    # the source ids need to stay as strings so I add those separately\n",
    "    num_column_names = ['ref_epoch', 'ra', 'ra_error', 'dec',\n",
    "                    'dec_error', 'parallax', 'parallax_error', 'parallax_over_error','pmra',\n",
    "                    'pmra_error', 'pmdec', 'pmdec_error',\n",
    "                    'radial_velocity', 'radial_velocity_error',\n",
    "                    'astrometric_params_solved', 'visibility_periods_used',\n",
    "                    'astrometric_sigma5d_max','ruwe',\n",
    "                    'phot_g_mean_mag', 'phot_g_mean_flux_over_error',\n",
    "                    'phot_bp_mean_mag', 'phot_bp_mean_flux_over_error',\n",
    "                    'phot_rp_mean_mag', 'phot_rp_mean_flux_over_error',\n",
    "                    'bp_rp','phot_bp_rp_excess_factor']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # we will have a pair of stars for each row\n",
    "    # this means we have two sets of query results in each row\n",
    "    # put the parameters in a dictionary with suffixes _a and _b to name columns accordingly\n",
    "    colname_dictionary = {}\n",
    "\n",
    "    for column in num_column_names:\n",
    "        colname_dictionary['{0}_a'.format(column)] = 0\n",
    "        colname_dictionary['{0}_b'.format(column)] = 0\n",
    "\n",
    "    # store all of the column names in colnames list\n",
    "    colnames = []\n",
    "    for entry in colname_dictionary:\n",
    "        colnames.append(entry)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" BUILD OUTPUT TABLES \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        # if the tables have already been started, open them and continue from where they left off\n",
    "        # this is a feature included because the code has a tendency to break before the process is completed\n",
    "        # we already open qrt earlier so leave that one commented\n",
    "\n",
    "\n",
    "        \n",
    "        query_results_table = Table.read('{0}/query_results_table_c{1}.ecsv'.format(save_path, core_num), header_start=0, data_start=1)\n",
    "        index_error_queries = Table.read('{0}/index_error_queries_c{1}.ecsv'.format(save_path, core_num), header_start=0, data_start=1)\n",
    "        try:\n",
    "            unknown_error_queries = Table.read('{0}/unknown_error_queries_c{1}.ecsv'.format(save_path, core_num), header_start=0, data_start=1)\n",
    "        except:\n",
    "            unknown_error_queries = Table(names = ('wds_identifier', 'wds_rownum'), dtype = ('a30', 'f8'))\n",
    "            pass\n",
    "\n",
    "\n",
    "        try:\n",
    "            next_WDS_id = query_results_table['wds_identifier'][len(query_results_table)-1]\n",
    "        except:\n",
    "            next_WDS_id = 0\n",
    "            pass\n",
    "\n",
    "        for row in range(len(query_results_table)):\n",
    "            try:\n",
    "                if query_results_table[row] == next_WDS_id:\n",
    "                    query_results_table.remove_row(row)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row in range(len(index_error_queries)-1):\n",
    "            if index_error_queries['wds_identifier'][row] == next_WDS_id:\n",
    "                index_error_queries.remove_row(row)\n",
    "\n",
    "        for row in range(len(unknown_error_queries)-1):\n",
    "            if unknown_error_queries['wds_identifier'][row] == next_WDS_id:\n",
    "                unknown_error_queries.remove_row(row)\n",
    "\n",
    "        query_results_table_rownum = max(len(query_results_table) - 1, 0)\n",
    "        index_error_queries_rownum = max(len(index_error_queries) - 1, 0)\n",
    "        unknown_error_queries_rownum = max(len(unknown_error_queries) - 1,0)\n",
    "\n",
    "        # checkpoint\n",
    "        print('previous tables read in')\n",
    "\n",
    "        #initialize wds identifier\n",
    "        wds_identifier = ''\n",
    "\n",
    "\n",
    "    except:\n",
    "            # the code above assumes that the tables exist... if they don't, we should make them\n",
    "\n",
    "            next_WDS_id = 0\n",
    "\n",
    "            # query results table will have all info for a pair of stars in one row\n",
    "            query_results_table = Table(names=colnames)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "            \n",
    "            # add the wds identifier column and source id columns (doesn't work until I add one row to the table)\n",
    "            # Lengths are not compatible as one as a row of 1 and query_rasults_table has a row = 0 before\n",
    "            # add_row()\n",
    "            # AttributeError when first add_column executed: AttributeError: 'str' object has no attribute 'info'\n",
    "            # Columns inserted as a dictionary with key:item and I think the Table expects the new column \n",
    "            # to have an attribute. I wsolved this by experimenting and found creating a Table.Column class \n",
    "            # and adding that with add_column() works. \n",
    "\n",
    "            # Add the WDS columnm information by creating columns and THEN adding these columns.\n",
    "            wds_id_col    =Table.Column(data=['                              '], name = 'wds_identifier')\n",
    "            wds_rownum_col=Table.Column(data=[00000000                        ], name='wds_rownum')\n",
    "            wds_s_ida_col =Table.Column(data=['                              '], name = 'source_id_a')\n",
    "            wds_s_idb_col =Table.Column(data=['                              '], name = 'source_id_b')\n",
    "        \n",
    "            # First add the rwo and then add the columns otherwise the row lengths are not \n",
    "            # the same.\n",
    "            query_results_table.add_row()\n",
    "            # Now use method add_column of Table class to add the WDS related columns.\n",
    "            query_results_table.add_column(wds_id_col, index=0)\n",
    "            query_results_table.add_column(wds_rownum_col, index=1)\n",
    "            query_results_table.add_column(wds_s_ida_col, index=2)\n",
    "            query_results_table.add_column(wds_s_idb_col, index=3)\n",
    "\n",
    "\n",
    "            # remove that first row -- the loop will add rows as needed\n",
    "            query_results_table.remove_row(0)\n",
    "\n",
    "\n",
    "\n",
    "            # index error wds info:\n",
    "            index_error_queries = Table(names = ('wds_identifier', 'wds_rownum'), dtype = ('a30', 'f8'))\n",
    "\n",
    "            # unknown error wds info:\n",
    "            unknown_error_queries = Table(names = ('wds_identifier', 'wds_rownum'), dtype = ('a30', 'f8'))\n",
    "\n",
    "\n",
    "\n",
    "            # initialize row numbers for each output table:\n",
    "            query_results_table_rownum = 0\n",
    "            index_error_queries_rownum = 0\n",
    "            unknown_error_queries_rownum = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #initialize wds identifier\n",
    "            wds_identifier = ''\n",
    "\n",
    "            print('new tables constructed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    LOOP THROUGH THE WDS \n",
    "    \"\"\"\n",
    "\n",
    "    # find starting rownum:\n",
    "    # this will re-do a few queries to make sure they are complete\n",
    "    new_start_row = int(query_results_table['wds_rownum'][-2])\n",
    "\n",
    "\n",
    "    # now: go through the wds from the rows designated for the particular core and query those objects in Gaia\n",
    "    for rownum in range(new_start_row, wds_end_row-1):\n",
    "        if row%5000==0:\n",
    "            print('Loop through WDS complete to row: ',row)\n",
    "\n",
    "        # If the previous WDS identifier (from last iteration of loop) is the same is current one,\n",
    "        # then this row was already accounted for in that previous query - so you can move on.\n",
    "        # This is because there are some systems that have 3+ components: all of them share a WDS identifier,\n",
    "        # and we query all components of a system a single iteration of the loop\n",
    "        if wdstab['WDS Identifier'][rownum] == wds_identifier:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # read in the wds identifier so we know which system is queried (this is how output files will connect to the input)\n",
    "            wds_identifier = wdstab['WDS Identifier'][rownum]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if there are multiple columns with same WDS identifier,\n",
    "        # query all of those objects and add them to gaiaresults list\n",
    "\n",
    "        # this code starts at the current rownum, and keeps going as long as the wds_identifiers of the future rows are the same as the current one\n",
    "        for shared_id_rownum in range(rownum, wds_end_row-1):\n",
    "            if wdstab['WDS Identifier'][shared_id_rownum] == wds_identifier:\n",
    "\n",
    "                print('\\n core # ', core_num, 'of ', total_cores, 'cores   --- row number: ', rownum)\n",
    "                \"\"\" make the 2 queries for given WDS row \"\"\"\n",
    "\n",
    "\n",
    "                # read in the coordinates for each of the WDS objects\n",
    "                ra1, dec1, ra2, dec2 =wdstab['RApri-prepped'][rownum], wdstab['DECpri-prepped'][rownum], wdstab['RAsec-prepped'][rownum], wdstab['DECsec-prepped'][rownum]\n",
    "\n",
    "                # query object 1\n",
    "                radius1 = 5*u.arcsec\n",
    "                coord = SkyCoord(ra=ra1 , dec = dec1, unit='deg')\n",
    "                myquery1 = query_gaia(coordinate=coord, radius=radius1)\n",
    "\n",
    "                # query object 2\n",
    "                radius2 = 5*u.arcsec\n",
    "                coord = SkyCoord(ra=ra2 , dec = dec2, unit='deg')\n",
    "                myquery2 = query_gaia(coordinate=coord, radius=radius2)\n",
    "\n",
    "\n",
    "                \"\"\" VERTICALLY STACK ALL QUERIES TO CREATE A LIST WITH ALL QUERIES FROM 1 WDS ROW \"\"\"\n",
    "\n",
    "                # first query for this WDS identifier: just add query 1 and 2 to list\n",
    "\n",
    "                if len(myquery1) + len(myquery2) == 0:\n",
    "                    index_error_queries.add_row()\n",
    "                    index_error_queries['wds_identifier'][index_error_queries_rownum] = wds_identifier\n",
    "                    index_error_queries['wds_rownum'][index_error_queries_rownum] = rownum\n",
    "                    index_error_queries_rownum +=1\n",
    "\n",
    "                    # checkpoint\n",
    "                    # print('index error table updated')\n",
    "                    pass\n",
    "                elif shared_id_rownum == rownum:\n",
    "                    gaiaresults = vstack([myquery1, myquery2])\n",
    "\n",
    "                # then, keep adding the new queries to the existing gaiaresults list\n",
    "                else:\n",
    "                    gaiaresults = vstack([gaiaresults, myquery1, myquery2])\n",
    "\n",
    "\n",
    "            # if WDS identifiers don't match, we have queried all component coordinates for the system -> move on\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            \"\"\" REMOVE DUPLICATES FROM GAIA RESULTS TABLE \"\"\"\n",
    "\n",
    "            # checkpoint\n",
    "            # print('length of gaiaresults is', len(gaiaresults))\n",
    "\n",
    "            gaiaresults = unique(gaiaresults, keep = 'first', silent = 'True')\n",
    "\n",
    "            # checkpoint\n",
    "            # print('duplicates_removed')\n",
    "            # print('length of gaiaresults is', len(gaiaresults))\n",
    "\n",
    "\n",
    "            # save all query results where less than two unique objects are found to index error query table\n",
    "            # if there's not at least two stars found, we can't do the analysis for that WDS system\n",
    "            if len(gaiaresults) <= 1:\n",
    "                index_error_queries.add_row()\n",
    "                index_error_queries['wds_identifier'][index_error_queries_rownum] = wds_identifier\n",
    "                index_error_queries['wds_rownum'][index_error_queries_rownum] = rownum\n",
    "                index_error_queries_rownum +=1\n",
    "\n",
    "                # update output table\n",
    "                ascii.write(index_error_queries, '{path}/index_error_queries_c{core}.ecsv'.format(path = save_path, core = core_num), format='ecsv',overwrite=True)\n",
    "\n",
    "                # checkpoint\n",
    "                # print('index error table updated')\n",
    "                pass\n",
    "\n",
    "\n",
    "            else: # if there's more than two objects found -> keep going!\n",
    "\n",
    "\n",
    "                \"\"\" CROSS CHECK EACH ENTRY WITH EACH OTHER \"\"\"\n",
    "                # avoid repeat comparisons\n",
    "\n",
    "                # make a list of every unique combination of two objects in my list\n",
    "                # this will be a comma separate string of source ids from Gaia\n",
    "                L = gaiaresults['source_id']\n",
    "                combolist = [\",\".join(map(str, comb)) for comb in combinations(L, 2)]\n",
    "\n",
    "                # checkpoint\n",
    "                # print('cross check complete')\n",
    "\n",
    "\n",
    "                #make source id column the index for gaiaresults table\n",
    "                # this allows us to return a row by searching the source id\n",
    "                gaiaresults.add_index('source_id')\n",
    "\n",
    "\n",
    "                # use the list of unique combinations and find both of those rows\n",
    "                # then, compare them\n",
    "\n",
    "\n",
    "\n",
    "                for combination in combolist:\n",
    "                # every unique combination of the gaiaresults is cross-compared \n",
    "                # to see if any pair of stars found around the WDS coordinates are gravitationally associated\n",
    "\n",
    "                    # the combination is a comma separated entry of two source ids -- unique combo\n",
    "                    # then, split them up so I can call to the data about each specific target in the combo\n",
    "                    # the source id is the index for my gaiaresults table, so I can call to the target row using the id\n",
    "                    query_a, query_b = combination.split(',')\n",
    "                    row_a = gaiaresults.loc[int(query_a)]\n",
    "                    row_b = gaiaresults.loc[int(query_b)]\n",
    "\n",
    "                    # checkpoint\n",
    "                    # print('components assigned')\n",
    "                    # print(gaiaresults)\n",
    "\n",
    "                    \"\"\" READ IN THE RELEVANT INFO (source id and parallax): \"\"\"\n",
    "\n",
    "                    # read in the parameters for object a and b\n",
    "                    # put the parameters in a dictionary with suffixes _a and _b accordingly\n",
    "                    parameter_dictionary = {}\n",
    "\n",
    "                    # go through all of the columns for both component a and b for each unique combo of gaiaresults\n",
    "                    # and update the output tables with the queried information as well as the corresponding WDS identifier\n",
    "\n",
    "                    # before updating the table, organize the information in a dictionary\n",
    "                    for column in query_results_table.colnames:\n",
    "                        if column == 'wds_identifier':\n",
    "                            parameter_dictionary['wds_identifier'] = wdstab[rownum]['WDS Identifier']\n",
    "                        # elif column == 'wds_rownum':\n",
    "                        #     parameter_dictionary['wds_rownum'] == rownum\n",
    "\n",
    "                        elif column.endswith('_a') == True:\n",
    "                            param_len = len(column)\n",
    "                            parameter_dictionary['{0}'.format(str(column))] = row_a[column[:param_len - 2]]\n",
    "                        elif column.endswith('_b') == True:\n",
    "                            param_len = len(column)\n",
    "                            parameter_dictionary['{0}'.format(str(column))] = row_b[column[:param_len - 2]]\n",
    "\n",
    "\n",
    "                    # make the next row for the query results output table\n",
    "                    query_results_table.add_row()\n",
    "\n",
    "                    # update the query results table with the info stored in the dictionary\n",
    "                    for entry in parameter_dictionary:\n",
    "                        query_results_table[entry][query_results_table_rownum] = parameter_dictionary[entry]\n",
    "\n",
    "                    # update wds_rownum separately because it wasn't working in the loop\n",
    "                    query_results_table['wds_rownum'][query_results_table_rownum] = rownum\n",
    "\n",
    "                    query_results_table_rownum +=1\n",
    "\n",
    "                    # update output file\n",
    "                    ascii.write(query_results_table, '{path}/query_results_table_c{core}.ecsv'.format(path = save_path, core = core_num), format='ecsv',overwrite=True)\n",
    "\n",
    "                    # checkpoint\n",
    "                    # print('query_results_table updated')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            # if an unexpected error occurs, add to table of objects with any error:\n",
    "            unknown_error_queries.add_row()\n",
    "            unknown_error_queries['wds_identifier'][unknown_error_queries_rownum] = wds_identifier\n",
    "            unknown_error_queries['wds_rownum'][unknown_error_queries_rownum] = rownum\n",
    "            unknown_error_queries_rownum +=1\n",
    "\n",
    "            # update output file\n",
    "            ascii.write(unknown_error_queries, '{path}/unknown_error_queries_c{core}.ecsv'.format(path = save_path, core = core_num), format='ecsv',overwrite=True)\n",
    "\n",
    "            #checkpoint\n",
    "            # print('unknown error')\n",
    "\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    # Hmm, do we need this again?\n",
    "    # save_path = '/home/student/djz7128/djz_NOFS/QueryResults'\n",
    "    save_path = os.getcwd()\n",
    "    \n",
    "    # write the output files to end in _c# where # is the core number that was used\n",
    "    ascii.write(query_results_table, '{path}/query_results_table_c{core}.ecsv'.format(path = save_path, core = core_num), format='ecsv',overwrite=True)\n",
    "    ascii.write(query_results_table, '{path}/query_results_table_c{core}.csv'.format(path = save_path, core = core_num), format='csv',overwrite=True)\n",
    "\n",
    "    ascii.write(index_error_queries, '{path}/index_error_queries_c{core}.ecsv'.format(path = save_path, core = core_num), format='ecsv',overwrite=True)\n",
    "    ascii.write(index_error_queries, '{path}/index_error_queries_c{core}.csv'.format(path = save_path, core = core_num), format='csv',overwrite=True)\n",
    "\n",
    "    ascii.write(unknown_error_queries, '{path}/unknown_error_queries_c{core}.ecsv'.format(path = save_path, core = core_num), format='ecsv',overwrite=True)\n",
    "    ascii.write(unknown_error_queries, '{path}/unknown_error_queries_c{core}.csv'.format(path = save_path, core = core_num), format='csv',overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b822db",
   "metadata": {
    "id": "20e98c53",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable total_num_queries is:  154513\n",
      "core:  1 end row =  154514\n",
      "new tables constructed\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -2 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-04c580f8d90a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sample tests... if you make the num_cores very high, then there's only a few queries for a process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwds_in_gaia_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# wds_in_gaia_query(1,30000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# wds_in_gaia_query(2,30000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-792383de86ae>\u001b[0m in \u001b[0;36mwds_in_gaia_query\u001b[0;34m(core_num, total_cores)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# find starting rownum:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# this will re-do a few queries to make sure they are complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mnew_start_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_results_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wds_rownum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mastropy/table/_column_mixins.pyx\u001b[0m in \u001b[0;36mastropy.table._column_mixins._ColumnGetitemShim.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mastropy/table/_column_mixins.pyx\u001b[0m in \u001b[0;36mastropy.table._column_mixins.base_getitem\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mastropy/table/_column_mixins.pyx\u001b[0m in \u001b[0;36mastropy.table._column_mixins.column_getitem\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -2 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# sample tests... if you make the num_cores very high, then there's only a few queries for a process\n",
    "\n",
    "wds_in_gaia_query(1,2)\n",
    "# wds_in_gaia_query(1,30000)\n",
    "# wds_in_gaia_query(2,30000)\n",
    "# wds_in_gaia_query(3,30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001102ed",
   "metadata": {
    "id": "fcf1e5aa-a7c0-49ed-ab0e-a5dc17413f69",
    "outputId": "3e44ca8e-f970-43be-9b22-0ce3d896ef14"
   },
   "outputs": [],
   "source": [
    "print('The length of the WDS Table is: ',len(wdstab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7cb59",
   "metadata": {
    "id": "2e100676"
   },
   "source": [
    "\n",
    "### Dividing up the WDS for multiprocessing\n",
    "##### this is incorporated in the main wds_in_gaia_query function, just rewritten here for checking this component of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb3b8e",
   "metadata": {
    "id": "0213e9b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Prepare for multiprocessing\n",
    "# total_cores = 2\n",
    "\n",
    "# # total number of queries will be the number of wds entries that we look at\n",
    "# total_num_queries = len(wdstab)\n",
    "\n",
    "# # find approx # of queries per core... ignoring the fraction\n",
    "# queries_per_core = total_num_queries // total_cores\n",
    "# leftover_rows = total_num_queries % total_cores\n",
    "\n",
    "# # make a list of the start and end row variables0\n",
    "# start_row_list = []\n",
    "# end_row_list = []\n",
    "\n",
    "# # make a list to get the start and end row for each process\n",
    "# rownum_counter = 0\n",
    "# for core in range(total_cores):\n",
    "#     start_row_list.append(rownum_counter)\n",
    "#     rownum_counter += queries_per_core\n",
    "#     if core == total_cores - 1:\n",
    "#         end_row_list.append(total_num_queries)\n",
    "#     else:\n",
    "#         end_row_list.append(rownum_counter)\n",
    "\n",
    "# end_row_list[-1] = end_row_list[-1] + leftover_rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d627a79",
   "metadata": {
    "id": "6c016f44"
   },
   "source": [
    "## Initiate Gaia Query with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47169f7",
   "metadata": {
    "id": "1995dbc2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initiate_gaia_query(total_cores):\n",
    "\n",
    "    # use multiprocsesing to query all objects in the WDS by splitting up the rows into separate processes that run concurrently\n",
    "\n",
    "\n",
    "    processes=[]\n",
    "\n",
    "    for core_num in range(total_cores):\n",
    "        print('process initiated: core', core_num)\n",
    "        p = multiprocessing.Process(target = wds_in_gaia_query, args = (core_num, total_cores))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c814557",
   "metadata": {
    "id": "31aa731a-773e-48eb-87dc-07ce636ce87c",
    "outputId": "8b018f84-34d2-4105-9da4-8d74ad600a2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#started back at 7:18 on 3/30\n",
    "# again at 9:10\n",
    "# agaian at 12:00\n",
    "#again at 9:51 3/31\n",
    "total_cores = multiprocessing.cpu_count()\n",
    "initiate_gaia_query(total_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12381236",
   "metadata": {
    "id": "cf145e98"
   },
   "source": [
    "## Test query multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6d600",
   "metadata": {
    "id": "f538c0a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def initiate_test_gaia_query():\n",
    "\n",
    "\n",
    "#     processes=[]\n",
    "\n",
    "#     # num_of_processes is the number of cores to use\n",
    "#     # divide_wds is a way to limit how many queries are done per core\n",
    "#     # there are around 150,000 wds entries,\n",
    "#     # so 150000/ divide_wds will be the number of queries per core\n",
    "\n",
    "#     num_of_processes = 4\n",
    "#     divide_wds = 300\n",
    "\n",
    "\n",
    "#     for core_num in range(num_of_processes):\n",
    "#         print('process initiated: core', core_num)\n",
    "#         p = multiprocessing.Process(target = wds_in_gaia_query, args = (core_num, divide_wds))\n",
    "#         p.start()\n",
    "#         processes.append(p)\n",
    "\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d02965",
   "metadata": {
    "id": "00061a3f",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a19b67",
   "metadata": {
    "id": "4864ccc5-2924-4106-850c-b0cabab9a161"
   },
   "source": [
    "## Figure out number of available cores in the comp (already set variable total_cores above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a001978",
   "metadata": {
    "id": "7e5ec9e4"
   },
   "outputs": [],
   "source": [
    "# multiprocessing. cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e218c",
   "metadata": {
    "id": "3f19e71b-2814-44c8-93ef-db060401d090"
   },
   "source": [
    "## Open the files that were saved in multiprocessing, save them to a single stacked table and a single stacked file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad01c1",
   "metadata": {
    "id": "1613ebd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The below line must have been set via a permanently coded answer based on the returned value\n",
    "# from multiprocessing.cpu_count() above\n",
    "# total_cores = 12\n",
    "# total_cores = multiprocessing.cpu_count()\n",
    "print('Number of cpu cores available: ',total_cores)\n",
    "\n",
    "file_dictionary = {}\n",
    "\n",
    "for core_num in range(total_cores):\n",
    "\n",
    "            file_dictionary['query_results_table_c{0}'.format(core_num)] = 0\n",
    "            file_dictionary['index_error_queries_c{0}'.format(core_num)] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "save_path = os.getcwd()\n",
    "\n",
    "\n",
    "for file in file_dictionary:\n",
    "\n",
    "    file_dictionary[file] = Table.read('{0}/{1}.ecsv'.format(save_path, file), header_start=0, data_start=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# vertically stack all 20 sections of each table\n",
    "\n",
    "\n",
    "query_results_table_list = []\n",
    "index_error_queries_list = []\n",
    "\n",
    "\n",
    "for file in file_dictionary:\n",
    "    if file.startswith('query_results_table_c'):\n",
    "        query_results_table_list.append(file_dictionary[file])\n",
    "    elif file.startswith('index_error_queries_c'):\n",
    "        index_error_queries_list.append(file_dictionary[file])\n",
    "\n",
    "\n",
    "\n",
    "stack_query_results_table = vstack(query_results_table_list)\n",
    "stack_index_error_queries = vstack(index_error_queries_list)\n",
    "\n",
    "\n",
    "ascii.write(stack_query_results_table, '{0}/stack_query_results_table.ecsv'.format(save_path), format='ecsv')\n",
    "ascii.write(stack_query_results_table, '{0}/stack_query_results_table.csv'.format(save_path), format='csv')\n",
    "\n",
    "\n",
    "ascii.write(stack_index_error_queries, '{0}/stack_index_error_queries.ecsv'.format(save_path), format='ecsv')\n",
    "ascii.write(stack_index_error_queries, '{0}/stack_index_error_queries.csv'.format(save_path), format='csv')\n",
    "\n",
    "\n",
    "qrt ='{0}/stack_query_results_table.ecsv'.format(save_path)\n",
    "ie = '{0}/stack_index_error_queries.ecsv'.format(save_path)\n",
    "stack_query_results_table = Table.read(qrt, header_start=0, data_start=1)\n",
    "stack_index_error_queries = Table.read(ie, header_start=0, data_start=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75d607",
   "metadata": {
    "id": "11c5ef91-f83c-43f4-97b5-f5a9e5687a8d",
    "outputId": "d801381e-eb75-49b7-e2a8-439dad0649ed"
   },
   "outputs": [],
   "source": [
    "stack_query_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c32d7d",
   "metadata": {
    "id": "1fa7c921-5625-4db0-b12f-a169d8d6f124",
    "outputId": "6ff9ca4c-1849-4163-9358-a8de4c7b22a3"
   },
   "outputs": [],
   "source": [
    "stack_index_error_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1db6c9",
   "metadata": {
    "id": "9ede8ab3-32cc-40ce-8b74-5f39b11fd382"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
